---
title: KNN-最近邻算法初探
permalink: knn-algorithm
date: 2020-04-03 12:40:36
tag: 
 - blog
 - 人工智能
 - 算法
categories: 人工智能
---


## 1. KNN 算法

KNN 算法属于监督学习的算法，基本原理是对整个数据整体进行打标签之后，对一个新的元素根据其在向量空间中的位置来对其分类。k近邻算法是在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，我们就说预测点属于哪个类。

KNN本质是基于一种数据统计的方法！其实很多机器学习算法也是基于数据统计的。KNN是一种memory-based learning，也叫instance-based learning，属于lazy learning。即它没有明显的前期训练过程，而是程序开始运行时，把数据集加载到内存后，不需要进行训练，就可以开始分类了。　具体是每次来一个未知的样本点，就在附近找K个最近的点进行投票。[参考](https://blog.csdn.net/sinat_35512245/article/details/55051306)

## 2. 重要参数

K 的取值、距离的度量选择、特征归一化

参考[【量化课堂】一只兔子帮你理解 kNN](https://www.joinquant.com/view/community/detail/a98b7021e7391c62f6369207242700b2)

## 3. 示例代码

尝试用经典的分类学习算法 KNN 最近邻（k-nearest neighbor ， 最简单的分类算法， 新的观测值的标签由 n 维空间中最靠近它的训练样本标签确定） 判断萼片长度和宽度、花瓣长度和宽度分别是 5.0cm, 3.0cm, 5.0cm, 2.0cm 的鸢尾花所属类别。

```python
import numpy  as np
from sklearn import datasets
iris = datasets.load_iris()

data = np.array([5.0, 3.0, 5.0, 2.0], dtype='float64')

k = 5

# 获得欧氏距离的数组
l2_array = [(iris.target[i], np.linalg.norm(data-iris.data[i])) for i in range(len(iris.data))]

# 取出最近的 k 个目标
topk = sorted(l2_array, key=lambda a: a[1])[:k]

# 找到个数最多的标签并输出
res = np.argmax(np.bincount([i[0] for i in topk ]))
iris.target_names[res]
```

## 参考资料

[1] [【量化课堂】一只兔子帮你理解 kNN](https://www.joinquant.com/view/community/detail/a98b7021e7391c62f6369207242700b2)

[2] [空间向量中各距离的意义](https://blog.csdn.net/tagsT/article/details/50214669)

[3] [机器学习之深入理解K-means、与KNN算法区别及其代码实现](https://blog.csdn.net/sinat_35512245/article/details/55051306)

[4] [一文搞懂k近邻（k-NN）算法（一）](https://zhuanlan.zhihu.com/p/25994179)介绍

[5] [一文搞懂k近邻（k-NN）算法（二）](https://zhuanlan.zhihu.com/p/26029567)KD树